# -*- coding: utf-8 -*-
"""Копия блокнота "GPT Professional. Lesson 15"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OO1meUPkULu5p5YvCW5pJfzjCW9f8Va-

Ссылка на архив с аудиозаписями [archive](https://drive.google.com/file/d/1QyfUZihJhVjM326EwJ9lm8oeTno2YgyI/view?usp=sharing). Скачать, распаковать, поместить к себе на гугл-диск в папку audio.

### Воспроизведение аудиофайлов
"""

from IPython.display import Audio

# воспроизведем аудио
Audio('/content/drive/MyDrive/audio/manager.mp3')

"""### Получение параметров аудио"""

# установка библиотеки pydub
# !pip install pydub

from pydub import AudioSegment

# чтение из файла любого формата
music = AudioSegment.from_file(file='/content/drive/MyDrive/audio/manager.mp3',
                               format='mp3')

# продолжительность аудио (секунды)
print(music.duration_seconds)

# частота дискретизаци
print(music.frame_rate)

# количество каналов
print(music.channels)

"""### Обрезка аудио"""

# чтение из файла любого формата
debaty = AudioSegment.from_file(file='/content/drive/MyDrive/audio/debaty.mp3',
                               format='mp3')

# выведем параметры
print(debaty.duration_seconds)
print(debaty.frame_rate)
print(debaty.channels)

# представление 40 секунд в миллисекундах
FORTY_SECONDS = 40 * 1000

# обрезка файла при помощи индексации
# возьмем первые 40 секунд аудиозаписи
debaty[:FORTY_SECONDS]

# сохраним фрагмент в файл
debaty[:FORTY_SECONDS].export('part.mp3', format='mp3')

"""# Google STT"""

# устанавливаем библиотеки
# !pip install SpeechRecognition

"""Библиотека SpeechRecognition одна из самых старых и простых в использовании. Она поддерживает большинство языков, но их нужно указывать явно. Поддерживает форматы WAV, AIFF, AIFF-C и FLAC."""

# воспроизведем аудио
Audio('/content/drive/MyDrive/audio/sample_3.wav')

import speech_recognition as sR

# путь к аудиофайлу
file_path = '/content/drive/MyDrive/audio/sample_3.wav'

# создаем объект класса Recognizer
r = sR.Recognizer()

# читаем файл
with sR.AudioFile(file_path) as source:
  audio = r.record(source)

# выполняем распознавание
result = r.recognize_google(audio, language='ru')
result

"""Мы можем ограничить аудиозапись по времени при помощи параметра `duration`"""

# читаем файл
with sR.AudioFile(file_path) as source:
  audio = r.record(source, duration=5)

# выполняем распознавание
result = r.recognize_google(audio, language='ru')
result

"""Если параметр `duration` отвечает за длительность распознаваемого отрезка (по умолчанию от 0), то при помощи параметра `offset` можно указать с какой секунды начать распознавание"""

# читаем файл
with sR.AudioFile(file_path) as source:
  audio = r.record(source, offset=5)

# выполняем распознавание
result = r.recognize_google(audio, language='ru')
result

# читаем файл
with sR.AudioFile(file_path) as source:
  audio = r.record(source, offset=2, duration=5)

# выполняем распознавание
result = r.recognize_google(audio, language='ru')
result

"""# Whisper AI

## Использование Whisper локально

Для использования Whisper локально достаточно установить библиотеку openai-whisper
"""

# установка библиотек
# !pip install -U openai-whisper

"""### Базовое применение"""

import whisper
from pprint import pprint

# пример базового применения
model = whisper.load_model("base")
result = model.transcribe("/content/drive/MyDrive/audio/manager.mp3")
print(result['text'])

# использование модели "medium"
model = whisper.load_model("medium")
result2 = model.transcribe("/content/drive/MyDrive/audio/manager.mp3")
print(result2['text'])

# использование модели "large"
model = whisper.load_model("large")
result3 = model.transcribe("/content/drive/MyDrive/audio/manager.mp3")
print(result3['text'])

# что ещё есть в 'result'
pprint(result3)

import pandas as pd

# запишем все сегменты в датафрейм для удобного просмотра
df = pd.DataFrame(result3["segments"])
df

"""### Автоматический перевод на английский язык"""

# использование модели 'large' с параметром task='translate'
model = whisper.load_model("large")
result = model.transcribe("/content/drive/MyDrive/audio/manager.mp3",
                          task="translate")
print(result['text'])

"""### Использование initial_prompt"""

# использование модели 'medium' с параметром 'initial_prompt'
model = whisper.load_model("medium")
result = model.transcribe("/content/drive/MyDrive/audio/manager.mp3",
                          initial_prompt='в рамках движения')
print(result['text'])

"""### Определение языка"""

import whisper

# загружаем базовую модель
model = whisper.load_model("base")

# загружаем аудио
audio = whisper.load_audio("/content/drive/MyDrive/audio/manager.mp3")

# извлекаем первые 30 секунд аудио (если аудио меньше, то добиваем нулями)
audio = whisper.pad_or_trim(audio)

# получаем log-Mel спектрограмму и передаем её на то же устройство, где находится модель
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# получаем словарь уверенностей распознанного языка
_, probs = model.detect_language(mel)
print(f"Распознанный язык: {max(probs, key=probs.get)}")

# выполняем распознавание речи
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# выводим полученный текст
print(result.text)

import matplotlib.pyplot as plt

# выведем log-Mel спектрограмму на экран
plt.figure(figsize=(20,10))
plt.imshow(mel.cpu())
plt.show()

# полученный словарь со значениями уверенности всех языков модели
probs

"""## Использование Whisper по API

Для использования Whisper по API необходимо иметь аккаунт openai и секретный токен OPENAI_API_KEY (это тот же токен, который используется для взаимодействия с chatgpt по API).
"""

# установка библиотек
# !pip install --upgrade openai

import openai
import getpass
import os

# передаем секретный апи токен
openai_key = getpass.getpass("OpenAI API Key:")
os.environ["OPENAI_API_KEY"] = openai_key
openai.api_key = openai_key

"""В API Whisper можно подавать файлы размером не более 25 МБ, формата `.mp3`, `.mp4`, `.mpeg`, `.mpga`, `.m4a`, `.wav`, и `.webm`.

### Базовое применение
"""

from openai import OpenAI

# создаем экземпляр класса OpenAI
client = OpenAI()

# выполняем чтение файла
audio_file= open("/content/drive/MyDrive/audio/manager.mp3", "rb")

# выполняем транскрибацию
transcript = client.audio.transcriptions.create(model="whisper-1",
                                                file=audio_file)

transcript.text

"""### Транскрибация + перевод"""

# создаем экземпляр класса OpenAI
client = OpenAI()

# выполняем чтение файла
audio_file= open("/content/drive/MyDrive/audio/manager.mp3", "rb")

# выполняем транскрибацию + перевод на английский язык
transcript = client.audio.translations.create(model="whisper-1",
                                                file=audio_file)

transcript.text

"""### Использование prompt

Подсказки (prompt) могут быть очень полезны для исправления специфических слов, аббревиатур, пунктуации, указания стиля письма  и тд.
"""

# создаем экземпляр класса OpenAI
client = OpenAI()

# открываем файл в режиме "rb"
audio_file= open("/content/drive/MyDrive/audio/manager.mp3", "rb")

# выполняем транскрибацию с параметром prompt
transcript = client.audio.transcriptions.create(model="whisper-1",
                                                file=audio_file,
                                                prompt='в рамках движения')

transcript.text

"""Ссылка на некоторые примеры использования prompt:

https://cookbook.openai.com/examples/whisper_prompting_guide

# Tinkoff VoiceKit

Для использования сервиса Tinkoff VoiceKit необходима предварительная регистрация на платформе https://software.tinkoff.ru/auth/login/

После регистрации на Вашем балансе будет 1000 рублей, на которые Вы можете безвозмездно использовать сервис.

После регистрации Вам необходимо создать и сохранить 2 ключа API-key и SECRET-key. API_KEY можно сгенерировать в личном кабинете в разделе VoiceKit в любой момент времени. SECRET_KEY генерируется автоматически только при получении первого API_KEY, потом SECRET_KEY будет недоступен, поэтому крайне ВАЖНО сразу его сохранить.
"""

# установка библиотек
# !pip install tinkoff-voicekit-client protobuf==3.20.3

import getpass

# передаем API_KEY
API_KEY = getpass.getpass("Tinkoff API Key:")

# передаем SECRET_KEY
SECRET_KEY = getpass.getpass("Tinkoff SECRET Key:")

"""## Метод **Recognize**

Используется для распознавания аудиофайлов разных форматов (mp3, wav, s16)

Для транскрибации текста, любой метод принимает словарь параметров:

    audio_config = {"encoding": "MPEG_AUDIO",  
                    "sample_rate_hertz": 16000,
                    "num_channels": 2}

    "encoding" - кодировка, может быть: 'LINEAR16', 'ALAW', 'MULAW', 'LINEAR32F', 'RAW_OPUS', 'MPEG_AUDIO';

    "sample_rate_hertz" - частота дискретизации записи;

    "num_channels" - количество каналов записи (1 или 2).
"""

from tinkoff_voicekit_client import ClientSTT
from pprint import pprint

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",
    "sample_rate_hertz": 16000,
    "num_channels": 2,
    }

# вызываем метод recognize
response = client.recognize("/content/drive/MyDrive/audio/manager.mp3",
                            audio_config)
pprint(response)

"""## Метод **StreamingRecognize**

Используется для распознавания речи в реальном времени: телефонных звонков, голосовых ассистентов и т.п. Этот метод содержит много разных фильтров и опций.
"""

# установка библиотеки ffmpeg-python
# !pip install ffmpeg-python

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",
    "sample_rate_hertz": 16000,
    "num_channels": 2,
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/manager.mp3", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""Метод StreamingRecognize позволяет использовать дополнительные опции в словаре параметров. Вот некоторые из них:

    audio_config = {"max_alternatives": 1,  
                    "vad_config": {},
                    "enable_automatic_punctuation": False,
                    "profanity_filter": False,
                    "enable_denormalization": False}

    "max_alternatives" - количество альтернатив распознанного текста, [1,15];

    "vad_config" - настройки для обнаружения голосовой активности;

    "enable_automatic_punctuation" - пунктуация;

    "profanity_filter" - фильтр ненормативной лексики;

    "enable_denormalization" - преобразование текста в числовые данные (время, дата и тп);

    ""

### Добавляем несколько альтернативных вариантов транскрибации текста
"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",   # кодировка
    "sample_rate_hertz": 16000, # частота
    "num_channels": 2,          # количество каналов
    "max_alternatives": 3       # количество альтернатив
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/manager.mp3", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""### Добавляем vad_config"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры vad (voice activity detection)
vad_config = {"silence_duration_threshold": 3,
              "silence_prob_threshold": 0.5}

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",   # кодировка
    "sample_rate_hertz": 16000, # частота
    "num_channels": 2,          # количество каналов
    "vad_config": vad_config    # vad параметры
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/manager.mp3", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""### Включаем пунктуацию"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",             # кодировка
    "sample_rate_hertz": 16000,           # частота
    "num_channels": 2,                    # количество каналов
    "enable_automatic_punctuation": True  # включаем пунктуацию
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/manager.mp3", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""### Фильтр ненормативной лексики

Фильтр выключен
"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "LINEAR16",      # кодировка
    "sample_rate_hertz": 8000,   # частота
    "num_channels": 1,           # количество каналов
    "profanity_filter": False    # фильтр ненормативной лексики
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/negative_sample.s16", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""Фильтр включен"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "LINEAR16",      # кодировка
    "sample_rate_hertz": 8000,   # частота
    "num_channels": 1,           # количество каналов
    "profanity_filter": True     # фильтр ненормативной лексики
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/negative_sample.s16", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)

"""### Преобразование текста в числа"""

# создаем клиент, передаем ключи
client = ClientSTT(API_KEY, SECRET_KEY)

# указываем параметры аудио
audio_config = {
    "encoding": "MPEG_AUDIO",        # кодировка
    "sample_rate_hertz": 48000,      # частота
    "num_channels": 1,               # количество каналов
    "enable_denormalization": True   # текстовые числа
}

# указываем параметры аудио
stream_config = {"config": audio_config}

# вызываем метод streaming_recognize
with open("/content/drive/MyDrive/audio/obratnyiy-otschet.mp3", "rb") as source:
    responses = client.streaming_recognize(source, stream_config)
    for response in responses:
        pprint(response)